% Palabras claves% Revision% Mejorara% Incorrecto% Cambio% Duda\documentclass[12pt,oneside]{book}\usepackage[utf8]{inputenc}\usepackage[spanish]{babel}\usepackage[bookmarks=false,colorlinks = true,linkcolor = blue,urlcolor  = blue,citecolor = blue,anchorcolor = blue]{hyperref}\usepackage[intlimits]{amsmath}%\usepackage{amsmath}\usepackage{amsfonts,amssymb,amsthm,extarrows}\makeatletter\def\th@plain{\thm@notefont{}\itshape }\def\th@definition{\thm@notefont{}\normalfont}\makeatother\usepackage[width=16cm,height=21cm]{geometry}\usepackage{graphicx}\usepackage{tikz}\usepackage{enumerate}\usepackage{colortbl}\usepackage{multicol}\usepackage{titlesec}\usepackage{comment}\usepackage{cite}\usepackage{apacite}% \usepackage{tabularx}\titleformat{\chapter}{\bfseries\huge}{\thechapter.}{20pt}{\bfseries\huge}\newtheorem{example}{Ejemplo}\newtheorem*{defn}{Definici\'on}\newtheorem{problem}{Problema}\newtheorem{teorema}{Teorema}\newtheorem{lema}{Lema}\newtheorem{prop}{Proposici\'on}\newcommand{\Enteros}{{\mathbb{Z}}}\newcommand{\Racionales}{{\mathbb{Q}}}\newcommand{\Reales}{{\mathbb{R}}}\newcommand{\Complejos}{{\mathbb{C}}}\renewcommand{\phi}{\varphi}\renewcommand{\epsilon}{\varepsilon}\newcommand{\matr}[2]{\left[\begin{array}{#1}#2\end{array}\right]}\newcommand{\sen}{\mathop{\mathrm{sen}}\nolimits}\newcounter{cejer}[section]\newenvironment{ejer}[1][\textbf{Ejercicios }]{#1\begin{enumerate}\setcounter{enumi}{\value{cejer}}}{\setcounter{cejer}{\value{enumi}}\end{enumerate}}\newenvironment{demo}[1][\underline{\textsl{Demostraci\'{o}n}}]{\textbf{#1. }}{\ \rule{0.5em}{0.5em}}\renewcommand{\chaptername}{ }\renewcommand{\contentsname}{Sumario}\renewcommand{\figurename}{Figura}\renewcommand{\max}{\text{m\'ax}}\renewcommand{\min}{\text{m\'in}}\renewcommand\bibname{Referencias}\everymath{\displaystyle}\begin{document}\begin{titlepage}\vspace{3cm}\begin{figure}[ht!]\centering\includegraphics[width=8cm]{ana}\end{figure}\hspace{.6\textwidth}\vspace{2cm}\begin{center}{\LARGE \textbf{An\'alisis de conglomerados y clasificaci\'on de series de tiempo. Utilizando Kullback-Liebler sobre la densidad espectral. Un ejemplo aplicado a audio de gatos y perros}}\break	\vspace{2cm} 		{\Large Facultad de Ciencias Actuariales}\\		{\Large Alumno: Campos Mart\'inez Joaqu\'in Nicol\'as}\\		{\Large Asesor: Dr. Cuevas Covarrubias Carlos}		\end{center}		\end{titlepage}%=====================================================================================================================% Indice%=====================================================================================================================\tableofcontents %Genera el indice\setcounter{page}{0}%=====================================================================================================================%Resumen ejecutivo%=====================================================================================================================\chapter*{Introducci\'on} % Este es un muy breve resumen de los conceptos claves que presentan relevancia con respecto a nuestro tema de inter\'es.\\% No para todos es claro lo que es una serie de tiempo, an\'alisis de conglomerados, y por supuesto una divergencia. Deseamos introducir cada concepto y su importancia. As\'i explicando la aplicabilidad de este proyecto.\\% Nuestro objetivo es crear un algoritmo, o una regla que se pueda utilizar como apoyo para decidir si una serie de tiempo es autorregresiva o de promedios m\'oviles.\\% Para crear esta regla nos basamos en an\'alisis discriminante, utilizando la medida de discrepancia de Kullback-Liebler. Est\'a requiere de funciones de densidad, por eso tomamos la funci\'on de densidad espectral de la serie de tiempo.\\% La funci\'on de densidad espectral est\'a en relaci\'on biyectiva con la funci\'on de autocorrelaci\'on parcial, por lo mismo hacer una comparaci\'on entre funciones de densidad espectral es comparar la estructura de varianza de las series.\\% La metodolog\'ia utilizada para obtener la funci\'on de densidad espectral se obtuvo del PriestleyEn este practicum queremos mostrar el uso de la densidad espectral para clasificar series de tiempo, algunas de las razones que impulsaron está idea fueron: primero algunos ejemplos, particularmente la clasificación de temblores basada en esta misma metodología\cite{SUBJ_SP1}\cite{SUBJ_SP2}, en el caso de Shumway como un medio para monitorear que se respeten las reglas internacionales de pruebas nucleares. Tambi\'en tenemos el caso de la clasificaci\'on de flamas seg\'un el sonido generado en la combusti\'on.\cite{SUBJ_SP3}\\Nuestro ejemplo en particular es la clasificaci\'on de audios de perros y gatos que forma parte de una base de datos de 48 clases ac\'usticas adicionales, utilizada en un art\'iculo sobre redes neuronales \cite{DATOS}.\\La base utilizada consta de 164 archivos WAV con sonidos de gatos, maullidos, ronroneos, o similares y 113 sonidos de perros, ladridos principalmente. Decidimos seguir el ejercio propuesto en el sitio Kaggle\cite{DATOS2}, donde se propone dividir la muestra en conjunto de entranamiento con 115 archivos de gatos y 64 de perros, los 49 restantes de cada categor\'ia se asignan al conjunto de prueba.\\Los resultados preliminares son bastante satisfactorios, y notamos que el espectro de maullidos es muy distinto de ladridos en t\'erminos de duraci\'on y harmon\'ia.\\Al final proponemos como ejercicio el posible uso en la clasificaci\'on de series de tiempo econ\'omicas, particularmente de precios de acciones o activos, de modo que podemos categorizar por estructuras de covarianza con un análisis de conglomerados, así podríamos decir que activos en un mismo grupo tienen un riesgo similar.\\% La idea del proyecto surge de un problema simple, tomemos en cuenta las canciones que cada quien escucha, no todos tenemos el talento para agruparlas en listas que suenen bien. De modo que la pregunta es ¿c\'omo introducimos un manera sistem\'atica para agrupar canciones?. No es un problema viejo, pero con la actual tecnolog\'ia y algoritmos, adem\'as del boom en servicios on-demand de m\'usica como Spotify\textcopyright el inter\'es en el tema ha incrementado.\\% La m\'usica se puede considerar como una serie de cambios en el tiempo, y la agrupaci\'on <<natural>> que buscamos de las canciones en listas de reproducci\'on es un problema que cae dentro de la categor\'ia de aprendizaje no supervisado, por lo mismo pensamos utilizar an\'alisis de conglomerados, una t\'ecnica exploratoria, cuya finalidad es encontrar estructuras homog\'eneas dentro de la muestra de observaciones. \\% En este momento, nosotros no proponemos una soluci\'on al problema, sino que nos enfocamos en problemas similares, series de tiempo finacieras, biom\'etricas, etc. y como solucionarlos. \\% Introducimos una medida de distancia (una divergencia en realidad), y queremos verificar que tan viable es.\setcounter{page}{0}\chapter{Marco Te\'orico} % (fold)\label{cha:marco_teorico}\section{Objetivo} % (fold)\label{sec:objetivo}% Nuestro objetivo es presentar una metodolog\'ia que sirva como apoyo en la selecci\'on de modelos de series de tiempo, la primera forma es la aplicaci\'on a problemas de clasificaci\'on y conglomeraci\'on' en distintos grupos utilizando la funci\'on de densidad espectral. La segunda aplicaci\'on es el uso para la selecci\'on de un modelo SARIMA sobre otro prospecto basandonos en la funci\'on de densiada espectral te\'orica.Nosotros queremos presentar una metodolog\'ia de clasificaci\'on y conglomeraci\'on de series de tiempo basada en la estructura de correlaci\'on. Nuestra base es utilizar la medida de discrepancia de Kullback-Liebler sobre la densidad espectral para clasificar en un grupo, seg\'un el espectro medio de cada grupo.% Nuestro objetivo es elaborar un algoritmo que sirva como herramienta de apoyo para distinguir entre series de tiempo con componente autorregresiva y con componente de promedios m\'oviles utilizando la divergencia de Kullback-Liebler sobre las funciones de densidad espectral como la medida de clasificaci\'on% de la serie de tiempo que se estudie.% section objetivo (end)\section{Relevancia} % (fold)\label{sec:relevancia}% El an\'alisis de conglomerados suele ser uno de los primero acercamientos en el an\'alisis exploratorio. Las series de tiempo pueden ser observadas en diferentes \'ambitos, informaci\'on de ventas, precios de acciones, tasas cambiarias, informaci\'on sobre el clima, datos biom\'edicos, etc. ~\cite{TIME_SERIES_CLUSTERING}\\\\Las aplicaciones de an\'alisis de conglomerados en estos \'ambitos son variadas, por ejemplo, en medicina es importante reconocer la diferencia entre se\~nales normales en un ECG, EEG, EMG de aquellas producidas por enfermedades. En sismolog\'ia, para discriminar las ondas s\'ismicas, de los moviemientos naturales de la tierra~\cite{TIME_SERIES_CLUSTERING_2} o para monitorear violaciones al CTBT (Tratado de prohibici\'on completa de los ensayos nucleares, en espa\~nol) \cite{SUBJ_SP1}.\\Otras aplicaciones son, en astronom\'ia las curvas de luz muestran el brillo de una estrella en un periodo de tiempo, en medicina la actividad cerebral, en el medio ambiente y urbanizaci\'on los niveles de la marea, niveles de contaminantes en el aire ($PM_{2.5}, PM_{10}$)~\cite{TIME_SERIES_CLUSTERING}.\\Otra raz\'on es la gesti\'on de portafolios de inversi\'on, donde se requiere diversificar el riesgo, y no tener muchas acciones de empresas con giros similares, o que esten altamente correlacionadas.% En ocasiones nos importa conocer series de tiempo que tienen comportamiento similar, por ejemplo en un portafolio de inversiones podemos tomar acciones de grupos cuyos elementos son similares, pero difieren de grupo a grupo.% En la pr\'actica, uno no sabe \textit{a priori} si una serie de tiempo tiene una componenente de promedios m\'oviles o una autorregresiva. En la metodolog\'ia Box-Jenkins no hay una forma clara de identificar esto, y nos basamos en las funciones de autocorrelaci\'on y autocorrelaci\'on parcial.\\% Nosotros queremos proponer un algoritmo que sirva como herramienta para comprobar si una ser\'ie de tiempo contiene componentes AR o MA.% section relevancia (end)\section{Antecedentes} % (fold)\label{sec:antecedentes}Hay m\'ultiples art\'iculos sobre an\'alisis de conglomerados, y algunos de ellos se enfocan en series de tiempo. \textit{Time-series clustering - A decade review}~\cite{TIME_SERIES_CLUSTERING} tiene como objetivo comparar los enfoques m\'as populares.\\<<This review will expose four main components of time-series clustering and is aimed to represent an updated investigation on the trend of improvements in efficiency, quality and complexity of clustering time-series approaches during the last decade and enlighten new paths for future works>>~\cite{TIME_SERIES_CLUSTERING}\\\textit{Clustering of time series data - a survey} busca resumir investigaci\'on realizada en el tema y sus aplicaciones en varios campos. Incluye aspectos basicos de los algoritmos m\'as generales que se utilizan en estudios de conglomerados, medidas de similitud y disimilitud, evaluaci\'on de conglomerados.~\cite{TIME_SERIES_CLUSTERING_3}\\Ambos art\'iculos distinguen 3 principales categorizaciones de los acercamientos a conglomeraci\'on de series de tiempo, trabajar con los datos brutos, extraer <<caracter\'isticas>> de los datos, basarse en un modelo al que se ajusten los datos. \\<<Computational Models of Music Similarity and their Application in Music Information Retrieval>> es una tesis que busca mostrar el desarrollo de las t\'ecnicas de descubrimiento de m\'usica basado en medidas de similitud y disimilitud. Uno de los enfoques consiste en utilizar de funci\'on de densidad espectral. Sin embargo tambi\'en utiliza t\'ecnicas aplicables a series de tiempo.~\cite{MUSIC}.% section antecedentes (end)\chapter{Como perros y gatos} % (fold)\label{cha:como_perros_y_gatos}\section{Distinguiendo su espectro} % (fold)\label{sec:distinguiendo_su_espectro}Basandonos en el ejemplo de \cite{EJEMPLO1}, decidimos tomar una observaci\'on de maullido cuyo sonido sea claro, y un ladrido tambi\'en n\'itido. La selecci\'on fue el archivo \textit{cat\_67.wav} y \textit{dog\_barking\_105.wav}. \\\begin{figure}[h]  \centering    \includegraphics[page=1,scale=.5]{resultados.pdf}  \caption{Espectograma maullido}  \label{plot:spec_maullido1}\end{figure}\begin{figure}[h]  \centering    \includegraphics[page=2,scale=.5]{resultados.pdf}  \caption{Espectograma ladrido}  \label{plot:spec_ladrido1}\end{figure}De los espectrogramas \ref{plot:spec_maullido1} y \ref{plot:spec_ladrido1} notamos que los maullidos son en tendencia harm\'onicos y tienen mayor duraci\'on que un ladrido de perro. Esta tendencia se sigue para los archivo que tienen menos ruido de fondo y son m\'as n\'itidos.\\\section{Densidad espectral} % (fold)\label{sec:densidad_espectral}El estimador que utilizamos de la funci\'on de densidad espectral es la correci\'on al periodograma con la ventana de Hann definida por$$w(n)=\frac{1}{2}\left(1-\cos\left(\frac{2\pi n}{N-1}\right)\right)$$Donde $N$ es el n\'umero de observaciones en la serie. De modo que el espectro queda representado por$$$$% $$\hat{f}(w)==\frac{1}{2\pi}\sum^{N-1}_{s=-(N-1)}I^*_{N}(w)(s)e^{isw}$$% Donde % $$\hat{R}(s)=\frac{1}{N}\sum^{N-|s|}_{t=1}(y_t-\bar{y})(y_{t+|s|}-\bar{y})$$% section densidad_espectral (end)'% Nuestro siguiente paso es obtener la funci\'on de densidad espectral, sin emabargo como est\'a se obtienen de todas las observaciones de la serie, necesitamos series con la misma duraci\'on, al menos para tener espectros en las mismas frecuencias. Para solucionar este problema se tomo la duraci\'on de la serie m\'as peque\~na (\emph{cat\_55.wav}) con poco menos de un segundo, y se tomo esa misma cantidad de tiempo alrededor del centro de cada serie de tiempo. De modo que cada serie es del mismo tama\~no.\\% Esto causa problemas en la clasificaci\'on del conjunto de entrenamiento como veremos adelante.\\\section{Clasifiaci\'on cortando las series} % (fold)\label{sec:clasifiacion_cortando_las_series}Los audios son de distinta duraci\'on, de modo que los espectros estimados ser\'an es distintas frecuencias, para solucionar este problema cortamos las series con la duraci\'on m\'as corta (\emph{cat\_55.wav}) de poco menos de un segundo. Partimos del punto medio de las mismas y tomamos aproximadamente medio segundo a la izquierda y medio a la derecha.% insertar ilustracion aqui\begin{figure}[h]  \centering    \includegraphics[page=1,scale=.5]{resultados2.pdf}  \caption{Densidad gatos 1}  \label{plot:dens_gato1}\end{figure}\begin{figure}[h]  \centering    \includegraphics[page=2,scale=.5]{resultados2.pdf}  \caption{Densidad perro 1}  \label{plot:dens_perro1}\end{figure}Una vez obtenida la densidad espectral, obtuvimos el espectro promedio por clase, la figura \ref{plot:dens_gato1} nos muestra la densidad espectral de los audios de gatos, podemos notar que tiene su moda en frecuencias bajas, y luego decae lentamente a 0. En cambio, la densidad espectral para ladridos, \ref{plot:dens_perro1}, tienen su moda en frecuencias medias, y decar r\'apidamente a 0. Ahora que tenemos nuestros espectros promedio, el siguiente paso es la regla de clasificaci\'on$$\text{clasifica en}\left\{\begin{array}{ccc}G &\text{si}&I(f_g, f)\leq I(f_p, f)\\P &\text{si}&I(f_g, f)>(f_p, f)\\\end{array}\right\}$$Donde $I(f, g)=\int_x f(x)\log(\frac{f(x)}{g(x)})dx$ es la informaci\'on de Kullback. La interpretaci\'on de $I$ es la informaci\'on media por observaci\'on en favor de $f$ contra $g$. En este caso nuestras $x$ son las frecuencias con las que estamos trabajando.Al aplicar est\'a regla obtenemos la matriz de confusi\'on para el conjunto de entrenamiento \ref{tbl:confusion_entre1}, donde nuestra tasa de error es del $17\%$. Para el conjunto de prueba nuestra matriz de confusi\'on nos indica una tasa de error del $26\%$\\\begin{tabular}{ccc} & Gato & Perro\\ Gatos & 77& 37\\Perro & 6 & 58\\\label{tbl:confusion_entre1}% \caption{Matriz de confusi\'on conjunto de entrenamiento}\end{tabular}\begin{tabular}{ccc} & Gato & Perro\\ Gatos & 36& 13\\Perro & 3 & 46\\\label{tbl:confusion_prueba1}% \caption{Matriz de confusi\'on conjunto de entrenamiento}\end{tabular}% section clasifiacion_cortando_las_series (end)% section distinguiendo_su_espectro (end)% chapter como_perros_y_gatos (end)%  \chapter{Algoritmo} % (fold)%  \label{cha:algoritmo}%  Para desarrollar el algoritmo se realizaron varias tareas, primero simular series de tiempo MA y AR, luego estimar la funci\'on de autocorrelaci\'on parcial, despu\'es obtener el periodograma (estimador de la funci\'on de densidad espectral). Luego se obtienen promedios de las funciones de densidad espectral \cite{SUBJ_SP1}, para poder utilizar la divergencia de Kullback-Liebler como una regla de decisi\'on para clasificar una serie de tiempo en el grupo correspondiente, est\'a es la metodolog\'ia propuesta por \cite{SUBJ_SP2}%  \section{Simulador de series de tiempo} % (fold)%  \label{sec:simulador_de_series_de_tiempo}%  R tiene integrado simuladores, pero por objevitos didactivos el simulador se realizo utilizando \'unicamente la funci\'on rnorm y la serie se contruye con el supuesto de que $\mu=0$ es decir nuestros modelos son de las siguientes formas% \begin{itemize}% 	\item AR $Y_t = \sum^n_{i=1}\phi_i Y_{t-i} + \epsilon_t$ donde $Y_i=0$ para $i=1, \dots, n$ y $\epsilon_t~norm(0,\sigma^2)$% 	\item MA $Y_t = \sum^n_{i=1}\theta_i \epsilon_{t-i} + \epsilon_t$ donde $\epsilon_i=0$ para $i=1-n, \dots, 0$ y $\epsilon_t~norm(0,\sigma^2)$ para $t=1, \dots, n$% 	\item $Y_t=\theta_1Y_{t-1}+\theta_2Y_{t-2}+\theta_{12}Y_{t-1}Y_{t-2}+\epsilon_t$ donde $Y_i=0$ para $i=1, 2$ y $\epsilon_t~norm(0, \sigma^2)$% \end{itemize}%  % section simulador_de_series_de_tiempo (end)%  % chapter algoritmo (end)% \section{Densidad espectral} % (fold)% \label{sec:densidad_espectral}% El estimador que utilizamos de la funci\'on de densidad espectral es el periodograma normalizado que se obtiene de la siguiente manera% $$\hat{f}(w)=\frac{I^*_{N}(w)}{\sigma^2_y}=\frac{1}{2\pi\sigma^2_y}\sum^{N-1}_{s=-(N-1)}\hat{R}(s)e^{isw}=\frac{1}{2\pi}\sum^{N-1}_{s=-(N-1)}\hat{\rho}(s)e^{isw}$$% Donde % $$\hat{R}(s)=\frac{1}{N}\sum^{N-|s|}_{t=1}(y_t-\bar{y})(y_{t+|s|}-\bar{y})$$% es el estimador de la funci\'on de autocovarianza, del mismo modo $\hat{\rho}(s)$ es el estimador de la funci\'on de autocorrelaci\'on. En ambos casos, el estimador utilizado no es el insesgado, esto es porque (priestley)\\% El periodograma es un estimador incosistente dado que para una serie de tama\~no $N$, los valores en $n$ de la funci\'on de autocorrelaci\'on (o autocovarianza), cuando $n$ se acerca a $N$ est\'an basados en pocas observaciones y tienden a tener mayor varianza. Aun as\'i es el estimador m\'as sencillo, y es la forma muestral de modelo te\'orico.\\% % section densidad_espectral (end)% \section{Clasificaci\'on} % (fold)% \label{sec:clasificacion}% Supongamos que tenemos $k$ distintas clases de series de tiempo, y que obtuvimos su funci\'on de densidad espectral media $\hat{f}_i(s)=\frac{1}{n_i}\sum^{n_i}_{j=1}\hat{f}_{ij}(s)$ donde  $i=1, \dots, k$\\% Clasificamos una serie de tiempo nueva $X(s)$, con funci\'on de densidad espectral $\hat{f_x}(s)$ en la clase $r$ si $I(\hat{f}_i,\hat{f}_x)\leq I(\hat{f}_r,\hat{f}_x)$, para $i=1,\dots,k$, donde $I$ es la medida de discrepancia de Kullback-Liebler dada por% $$I(f, g)=\int_x f(x)\log(\frac{f(x)}{g(x)})dx$$% Para nuestro caso utilizamos la forma discreta $I(\hat{f}_i, \hat{f}_x)=\sum^N_{j=1} \hat{f}_i(w_j)\log(\frac{\hat{f}_i(w_j)}{\hat{f}_x(w_j)})dx$% section clasificacion (end)% chapter marco_teorico (end)% \chapter{Alcance del proyecto} % (fold)% \label{cha:alcance_del_proyecto}% \section{Metodolog\'ia} % (fold)% \label{sec:metodologia}% Ya mencionamos los conceptos principales detr\'as del proyecto, \emph{series de tiempo}, \emph{an\'alisis espectral}, \emph{divergencia de Kullback-Liebler} y \emph{an\'alisis de conglomerados}. La meta del proyecto es utilizar los conceptos anteriores sobre distintas series de tiempo, precios de acciones, clima, \emph{m\'usica}, etc y analizar los resultados obtenidos. \\% Nuestra propuesta es que utilizar la divergencia de Kullback-Liebler como medida de distancia, y comparar con respecto de otros acercamientos a la conglomeraci\'on de series de tiempo.\\% Nos centraremos en conglomerados jer\'arquicos, sin embargo con la idea detras de la divergencia de Kullback-Liebler, la aplicaci\'on de k-medoides suena como una idea intuitiva, donde queremos minimizar la divergencia de los elementos de un conglomerado con respecto de un <<centroide>> (medoide).\\% La informaci\'on a utilizar ser\'a extra\'ida de Yahoo finance o un servicio similar, con fines puramente did\'acticos. Eventualmente deseamos crear un algoritmo y aplicaci\'on que permita observar los resultados obtenidos de la aplicaci\'on de un algoritmo basado en las ideas de este proyecto. \\% La misma idea desea ser aplicada, sobre series de tiempo basadas en m\'usica, pero no se consider\'a dentro del alcance de este trabajo.\\% Para el an\'alisis de datos se utilizar\'a el software R, y las librer\'ias tuneR y seewave, aunque no se descartan otras aplicaciones que puedan ser m\'as \'utiles. \\% Por cuestiones de optimizaci\'on, no se utilizar\'an las series completas, sino que se reduciran de manera que sean comparables, longitudes iguales y a tiempos iguales.% % section metodologia (end)% \section{Fuentes de datos} % (fold)% \label{sec:fuentes_de_datos}% La cantidad de informaci\'on disponible actualmente es impresionante, y se puede obtener de distintas formas, mediante bases de datos online, minando informaci\'on, o simplemente checando el registro de nuestras acciones. \\% Nuestra principal fuente son los precios de acciones, adquiridos mediante Yahoo Finance \textcopyright. Y en la medida de lo posible se buscar\'a utilizar otras bases de datos disponibles. Y tambi\'en archivos de audio, m\'usica, obtenidos mediante compra en distintos servicios iTunes Store\textcopyright, Bandcamp\textcopyright, o adquiridos gratuitamente.% Algunos de los ejemplos que realizaremos ser\'an estilo <<libro>> utilizando bases de datos populares disponibles en el sitio web de la \emph{International Association for Statistical Computing}% Para efectos de este estudio, los datos se obtendr\'an de distintas fuentes, principalmente de archivos de audio, mp3, representando m\'usica obtenidos mediante compra en distintos servicios iTunes Store\textcopyright, Bandcamp\textcopyright, o adquiridos gratuitamente. Los datos se extraer\'an mediante con la ayuda de 2 librer\'ias del R <<tuneR>> y <<seewave>>. Las cu\'ales contienen funcionalidad para trabajar con ondas, archivos de m\'usica, etc. Otra fuente es los precios de acciones en adquiridos mediante Yahoo Finance \textcopyright\\\\% section fuentes_de_datos (end)% \section{Pasos siguientes} % (fold)% \label{sec:expectativas_practicum_ii}% \begin{itemize}% 	\item 13-Sep Presentaci\'on de los avances, Actualizaci\'on del escrito% 	\item 20-Sep Anexos de los temas complejos% 	\item 27-Sep Implementar en c\'odigo el clasificador% 	\item 4-Oct Experimentos y resultados con series AR y MA simuladas% 	\item 11-Oct Experimentos y resultados con series reales% 	\item 18-Oct Correcciones y actualizaci\'on de escrito % 	\item 25-Oct Verificaci\'on de escrito, sobre detalles finales% 	\item 1-Nov Entregar borrador de presentación y escrito a sinodales% 	\item 8-Nov Dar dise\~no a cartel (Im\'agenes, gr\'aficas y diseño en general)% 	\item 15-Nov Ajustes Finales para presentaci\'on de cartel% 	\item 22-Nov Detallar por Completo escrito Final % 	\item 29-Nov Detallar por Completo escrito Final % \end{itemize}% section expectativas_practicum_ii (end)% chapter alcance_del_proyecto (end)%=====================================================================================================================%Chapter 1 {An\'alisis de conglomerados}%=====================================================================================================================% \chapter{An\'alisis de conglomerados} % % Representing data by fewer clusters necessarily loses certain fine details (akin to lossy data compression), but achieves simplification. It represents many data objects by few clusters, and hence, it models data by its clusters.Data modeling puts clustering in a historical perspective rooted in mathematics, statistics,and numerical analysis. From a machine learning perspective clusters correspond to hidden patterns, the search for clusters is unsupervised learning, and the resulting system represents a data concept. Therefore, clustering is unsupervised learning of a hidden data concept. Data mining deals with large databases that impose on clustering analysis additional severe computational requirements. These challenges led to the emergence of powerful broadly applicable data mining clustering methods surveyed below.% %=====================================================================================================================% %Section 1.1 {Tema}% %=====================================================================================================================% \section{Tema}% El tema general es \textit{an\'alisis de conglomerados}, con un enfoque a \textbf{series de tiempo}. Es decir la agrupaci\'on de series de tiempo en diferentes grupos seg\'un su <<similitud>>. % \\\\% Una de las principales motivaciones fue el plantemiento de un problema de unsupervised pattern recognition, el cual consiste en lo siguiente. Dado un grupo de canciones, deseamos crear listas de reproducci\'on que suenen bien. Por sonar bien, queremos decir que las listas de reproducci\'on presenten <<sinergia>>.% \\\\% Nuestro acercamiento consiste en tomar las funci\'on de \textbf{densidad espectral} varias series de tiempo, o una fracci\'on de las mismas; y luego aplicar la divergencia de \textbf{Kullback-Liebler} como m\'edida de distancia entre las funciones de densidad. Y luego aplicar alg\'un algoritmo jerarquico de aglomeraci\'on.% Podemos interpretar a la m\'usica como una serie de tiempo, y podmeos realizar un acercamiento por metodolog\'ias aplicables a series de tiempo. Cabe mencionar esta no es la \'unica aplicaci\'on. %=====================================================================================================================%Section 1.2 {Objetivo}%=====================================================================================================================% \section{Objetivo}% Deseamos probar la calidad de una metodolog\'ia para conlglomerar series de tiempo basada en \textbf{an\'alisis espectral} y la \textbf{divergencia de Kullback-Liebler} como medida de distancia entre series de tiempo. En nuestro caso se utilizar\'a un m\'etodo de conglomeraci\'on jer\'arquico. Y deseamos conocer si este enfoque posee ventajas intr\'insecas sobre otras metodolog\'ias de conlgomeraci\'on de series de tiempo, o si revela informaci\'on relevante sobre las series de tiempo.%Mejora Buscar sobre pruebas de calidad sobre los clusters%=====================================================================================================================%Section 1.3 {Relevancia}%=====================================================================================================================%=====================================================================================================================%Chapter 2 {Antecedentes}%=====================================================================================================================%=====================================================================================================================%Bibliograf\'ia%=====================================================================================================================\bibliography{Tar}{}\bibliographystyle{apacite}\chapter*{Anexos A: Teor\'ia} % (fold)\label{cha:anexos}\section{Series de tiempo} % (fold)\label{sec:series_de_tiempo}Una simplificaci\'on de la definici\'on de una serie de tiempo es: un proceso que var\'ia en el tiempo, observaciones tomadas secuencialmente en el tiempo. En la vida real podemos observar varios fen\'omenos de este tipo, como los mencionados arriba.\begin{defn}\normalfont  Una serie de tiempo es una colecci\'on de de observaciones hechas en una secuencia de tiempo. Se dice que una serie de tiempo es continua si las observaciones son en tiempos continuos. Se dice discreta si cuando las observaciones se toman en tiempos espec\'ificos, usualmente separados a intervalos iguales\end{defn}% section series_de_tiempo (end)\section{An\'alisis de conglomerados} % (fold)\label{sec:analisis_de_conglomerados}<<Clustering is a division of data into groups of similar objects. Each group, called cluster, consists of objects that are similar between themselves and dissimilar to objects of other groups.>>~\cite{CLUSTER_DEFINITION}\\An\'alisis de conglomerados no es una t\'ecnica asociada a un \'unico tipo de problemas. Pero es asociado naturalmente a las ideas de grupos homog\'eneos, clases de equivalencia, datos multimodales.\\Punj \& Stewart identifican algunas de las principales aplicaciones en el campo de investigaci\'on de mercado como segmentaci\'on de mercado, identificaci\'on de grupos homog\'eneos de consumidores, desarrollo de nuevos productos potenciales, selecci\'on de un mercado de prueba, como t\'ecnica general de reducci\'on.\\Las t\'ecnicas aplicadas son varias, y algunas de las m\'as tratadas en la literatura son conglomerados jer\'arquicos, y algunos algoritmos de k-centroides.\\Tenemos que reconocer que nuestro objeto de estudio son observaciones multivariadas, medidas en $p$ variables, que pueden ser categ\'oricas (respuestas a una encuesta, s\'i, no) o num\'ericas (valores continuos, como estatura y peso)\subsection{Conglomerados Jer\'arquicos} % (fold)\label{sub:conglomerados_jerarquicos}Se separa en 2 grandes categor\'ias, aglomerativos y divisivos. El primer tipo considera cada observaci\'on, elemento de la muestra, como un conglomerado y comienza a agruparlos basado en alguna regla. El segundo comienza con toda la muestra como un gran conglomerado y comienza a separarlo en distintos conglomerados, en su mayor\'ia conforme a una funci\'on objetivo. Un an\'alisis de conglomerados jer\'arquico es sensible, o depende principalmente de 2 factores.\begin{itemize}	\item La medida de \textbf{similitud o distancia} elegida	\item El algoritmo para agrupar elementos, la medida de \textbf{distancia o similitud} entre conglomerados\end{itemize}Conglomerados jer\'arquicos aglomerativos suelen ser los t\'ipicos ejemplos de libro de texto por su facilidad para explicarse, sin embargo esto no significa que su utilidad sea menor.Primero debemos abordar el tema de distancia o simulitud. \begin{defn}\normalfont	Dado un conjunto $X$ una medida de distancia es una funci\'on $d:X\times X\rightarrow\Reales$ tal que cumple las siguientes propiedades	\begin{itemize}		\item $d(x, y) = 0$ si y s\'olo si $x = y$		\item $d(x, y) = d(y,x)$ para todas $x, y \in X$		\item $d(x, y) + d(y, z) \geq d(x, z)$ para todas $x, y, z \in X$	\end{itemize}	La propiedad de no negatividad $d(x, y) \geq 0$ para todas $x, y \in X$ queda definida por las propiedades anteriores.\end{defn}La idea de distancia suele ser intuitiva, y se asocia com\'unmente con la distancia euclidiana, sin emabargo existen muchas otras formas de distancia, como la suma de valores absoulutos de las componentes. En cambio, la simulitud, aunque la idea es intuitiva, no suele ser tan clara cuando uno maneja valores num\'ericos o cateog\'oricos.\\Exiten algunos intentos sobre la definici\'on de una medida de similitud, entre ellos mencionaremos como ejemplo la definic\'on presentada por Chen, Ma y Zhang ~\cite{CLUSTER_ANALYSIS_2}\begin{defn}\normalfont	Dado un conjunto $X$ una medida de similitud es una funci\'on $s:X\times X\rightarrow\Reales$ que cumple las siguientes propiedades	\begin{itemize}		\item $s(x, y) = s(y, x)$ para todo $x, y\in X$		\item $s(x, x) \geq 0$ para todo $x\in X$		\item $s(x, x) \geq s(x, y)$ para todo $x, y\in X$		\item $s(x, y) + s(y, z) \leq s(x, z) + s(y, y)$ para todo $x, y, z \in X$		\item $s(x, x) = s(y,y) = s(x, y)$ si y s\'olo si $x=y$	\end{itemize}\end{defn}La propiedad de simetr\'ia es intuitiva, se espera que un objeto sea tan similar a otro, como el otro al primero. La segunda propiedad no es necesaria, pero su importancia radica en el concepto que tenemos de similitud, un objeto no puede tener similitud negativa con respecto de si mismo. La tercer propiedad parte de la idea de que un ning\'un obteto es tan similar a otro como a s\'i mismo. \\La cuarta propiedad ser\'ia el equivalente a la desigualdad del tri\'angulo, y aunque a primera vista parece poco comprensible, es m\'as f\'acil analogarla con la idea de intersecci\'on de conjuntos, podemos decir que $s$ es una medida de todo lo que tienen en com\'un 2 objetos $x, y$. La \'ultima propiedad nos dice que s\'olo cuando 2 objetos son iguales se alcanzan las cuotas superiores dela similitud.\\No es importante recordar esta definici\'on de similitud, pues nosotros trabajaremos algo m\'as parecido a una distancia que a una similitud.\\Lo siguiente es construir una matriz de distancias o similitudes entre los objetos. Si denotamos $s_{ij} = s(x_i, x_j)$ y $d_{ij} = d(x_i, x_j)$, y tenemos un total de $n$ observaciones, nuestra matriz de similitud es $$	\left[\begin{array}{cccc}		s_{11} &  s_{12} & \cdots & s_{1n}\\		s_{21} &  s_{22} & \cdots & s_{2n}\\		\vdots & \vdots & \ddots & \vdots\\		s_{n1} &  s_{n2} & \cdots & s_{nn}\\	\end{array}\right]$$y nuesta matriz de distancias es $$	\left[\begin{array}{cccc}		d_{11} &  d_{12} & \cdots & d_{1n}\\		d_{21} &  d_{22} & \cdots & d_{2n}\\		\vdots & \vdots & \ddots & \vdots\\		d_{n1} &  d_{n2} & \cdots & d_{nn}\\	\end{array}\right]$$En un algoritmo aglomerativo el siguiente paso es juntar las 2 observaciones, distintas, m\'as similares (menos distantes) para formar un conglomerado, luego se recalcula la distancia del resto de los conglomerados (observaciones) al conglomerado reci\'en formado. \\\textbf{¿C\'omo se calcula la distancia entre 2 conglomerados?}. Existen distintas reglas para calcular la distancia, algunas de las m\'as conocidas son\begin{itemize}	\item Vecino m\'as cercano (single linkage), es decir, la distancia entre 2 conglomerados es la distancia m\'as corta de sus elementos	\item Vecino m\'as lejano (complete linkage), es decir, la distancia entre 2 conglomerados es la distancia m\'as grande entre sus elementos	\item Promedio (average linkage), media de las distancias entr los elementos de ls conglomerados.	\item Centroide, se obtiene el centro de masa de los conglomerados y se calcula la distancia entre los mismo\end{itemize}Este proceso se repite hasta tener toda la muestra en un conglomerado, y cuando uno decida hacer el corte, cuando hay $r$ conglomerados, donde el usuario escoge $r$.% subsection conglomerados_jerarquicos (end)\subsection{K-centroides} % (fold)\label{sub:k_centroides}Los algoritomos de k-centroides est\'an basados en la minimizaci\'on de una funci\'on de suma de cuadrados, es decir la suma de cuadrados de la distancia de los elementos de un conglomerado a su centroide.En comparaci\'on con un an\'alsis jer\'arquico, k-centroides depende de 2 configuraciones\begin{itemize}	\item La primera es el n\'umero de conglomerados, $k$	\item El tipo de centroide, centro de gravedad, medoide.\end{itemize}  En el caso de \textbf{k-means}, el centroide es el centro de gravedad y   $$\begin{array}{ccc}E &=& \sum_{h=1}^k \sum_{i=1}^n u_{i,h} d(x_i, \mu_h)^2\end{array}$$Es la funci\'on que deseamos optimizar~\cite{CLUSTERING_1}. Donde los $u_{ih}$ clumplen que $\sum_{h=1}^k u_{ih}=1$ para $i = 1, \dots, n$ y $\sum_{i=0}^n u_{ih}=1$ para $h = 1, \dots, k$, y $\mu_h$ es la la media del $h$-\'esimo conglomerado. En el caso de medoides sustituimos $\mu_h$ por $m_h$, donde $m_h$ es el medoide del $h$-\'esimo.\\Hasta ahora s\'olo hemos presentado metodolog\'ias en las que una observaci\'on no puede ser parte de m\'as de un conglomerado sin embargo existen otros algoritmos que asignan «grados de pertenencia» en lugar de un conglomerado fijo.% subsection k_centroides (end)% section analisis_de_conglomerados (end)\section{An\'alisis espectral} % (fold)\label{sec:analisis_espectral}Inferencias basadas en la funci\'on de densidad espectral se dice que es un an\'alisis in el Dominio de las frecuencias.\begin{teorema}[Wiener-Khintchine]\normalfontPara cualquier proceso estoc\'astico estacionario con funci\'on de autocovarianza $\gamma(k)$ existe una funci\'on monotamente creciente $F(W)$ tal que $$\gamma(k) = \int^\pi_0 \cos(wk)dF(w)$$Llamada la representaci\'on espectral de la funci\'on de autocovarianza, que involucra un tipo de integral llamada de Stieltjes\end{teorema}$F(w)$ tiene una interpretaci\'on f\'isica directa, es la contribuci\'on a la varianza que se puede atribuir a las frecuencias en el rango $(0,w)$$F(w)$ es mon\'otona en el intervalo $(o,\pi)$, as\'i se puede descomponer en 2 funciones $F_1(w)$ y $F_2(w)$ talque $$F(w) = F_1(w) + F_2(w) $$ Donde $F_1(w)$ es una funci\'on continua no decreciente y $F_2(w)$ es una funci\'on escalonada no decreciente(WOLD descomposition). Adem\'as $F_1(w)$ se relaciona con la componente puramente no determinista del proceso y $F_2(w)$ se relaciona con la componente determinista. Nuestro estudio es sobre los procesos puramente indeterminados, donde $F_2(w) = 0$, de tal forma que $F(w)$ es continua en $(0, \pi)$.La potencia est\'a directamente relacionado al cuadrado de la amplitud de oscilaci\'on. y la varianza es la potencia total\begin{defn}\normalfontLa forma normalizada de $F(w)$ est\'a dada por$$F^* = \frac{F(w)}{\sigma_x^2}$$De modo que $F^*(w)$ es la proporci\'on de varianza atribuida por las frecuencias en el rango $(0,w)$. Como $F^*(\pi) = 1$ y es adem\'as mon\'ona creciente tenemos que $F^*(w)$ tiene propiedades similares a una funci\'on de distribuci\'on acumulada. ~\cite{TIME_SERIES_2}\end{defn}% section analisis_espectral (end)\section{Kullback-Liebler} % (fold)\label{sec:kullback_liebler}\begin{defn}\normalfont	Dadas dos funciones de densidad de probabilidad $f, g$ definidas sobre un mismo espacio medible, y que son absolutamente continuas entre s\'i, la divergencia de Kullback-Liebler se define como	$$		d_{KL}(f, g) = \int f(x) \ln{\left(\frac{f(x)}{g(x)}\right)}dx	$$\end{defn}La divergencia de Kullback-Liebler como una medida de divergencia dirigida. Una manera de entender lo anterio es primero considerar un modelo verdadero $f$ y un modelo de aproximaci\'on $g$, entonces la divergencia de Kullback-Liebler nos habla de cuanta informaci\'on se pierde utilizando $g$ como aproximaci\'on ~\cite{KULLBACK}. La divergencia de Kullback-Liebler no cumple con las propiedades de distancia mencionadas anteriormente.\\La propiedad m\'as cercana a una distancia, es la positividad y la nulidad cuando $f$ y $g$ son iguales exceptuando en puntos de probabilidad $0$.\\Ahora podemos definir la divergencia de Jeffreys\begin{defn}\normalfont	Dadas dos funciones de densidad de probabilidad $f, g$ definidas sobre un mismo espacio medible, y que son absolutamente continuas entre s\'i, la divergencia de Jeffreys se define como	$$		d_{J}(f, g) = d_{KL}(f, g) +d_{KL}(g, f) = \int (f(x)-g(x)) \ln{\left(\frac{f(x)}{g(x)}\right)}dx	$$\end{defn}En este caso, tenemos dos modelos, y la divergencia de Jeffreys nos indica la dificultad para distinguir entre ambos~\cite{KULLBACK_2}. Esta nueva medida de divergencia soluciona el problema de la sim\'etria'% section kullback_liebler (end)% chapter anexos (end)\chapter{Anexo B: C\'odigo} % (fold)\label{cha:codigo}% chapter codigo (end)\end{document}