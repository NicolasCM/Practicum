% Palabras claves% Revision% Mejorara% Incorrecto% Cambio% Duda\documentclass[12pt,oneside]{book}\usepackage[utf8]{inputenc}\usepackage[spanish]{babel}\usepackage[bookmarks=false,colorlinks = true,linkcolor = blue,urlcolor  = blue,citecolor = blue,anchorcolor = blue]{hyperref}\usepackage[intlimits]{amsmath}%\usepackage{amsmath}\usepackage{amsfonts,amssymb,amsthm,extarrows}\makeatletter\def\th@plain{\thm@notefont{}\itshape }\def\th@definition{\thm@notefont{}\normalfont}\makeatother\usepackage[width=16cm,height=21cm]{geometry}\usepackage{graphicx}\usepackage{tikz}\usepackage{enumerate}\usepackage{colortbl}\usepackage{multicol}\usepackage{titlesec}\usepackage{comment}\usepackage{cite}\usepackage{apacite}% \usepackage{tabularx}\titleformat{\chapter}{\bfseries\huge}{\thechapter.}{20pt}{\bfseries\huge}\newtheorem{example}{Ejemplo}\newtheorem*{defn}{Definici\'on}\newtheorem{problem}{Problema}\newtheorem{teorema}{Teorema}\newtheorem{lema}{Lema}\newtheorem{prop}{Proposici\'on}\newcommand{\Enteros}{{\mathbb{Z}}}\newcommand{\Racionales}{{\mathbb{Q}}}\newcommand{\Reales}{{\mathbb{R}}}\newcommand{\Complejos}{{\mathbb{C}}}\renewcommand{\phi}{\varphi}\renewcommand{\epsilon}{\varepsilon}\newcommand{\matr}[2]{\left[\begin{array}{#1}#2\end{array}\right]}\newcommand{\sen}{\mathop{\mathrm{sen}}\nolimits}\newcounter{cejer}[section]\newenvironment{ejer}[1][\textbf{Ejercicios }]{#1\begin{enumerate}\setcounter{enumi}{\value{cejer}}}{\setcounter{cejer}{\value{enumi}}\end{enumerate}}\newenvironment{demo}[1][\underline{\textsl{Demostraci\'{o}n}}]{\textbf{#1. }}{\ \rule{0.5em}{0.5em}}\renewcommand{\chaptername}{ }\renewcommand{\contentsname}{Sumario}\renewcommand{\figurename}{Figura}\renewcommand{\max}{\text{m\'ax}}\renewcommand{\min}{\text{m\'in}}\renewcommand\bibname{Referencias}\everymath{\displaystyle}\begin{document}\begin{titlepage}\vspace{3cm}\begin{figure}[ht!]\centering\includegraphics[width=8cm]{ana}\end{figure}\hspace{.6\textwidth}\vspace{2cm}\begin{center}{\LARGE \textbf{An\'alisis de conglomerados sobre series de tiempo. Utilizando una modificiaci\'on de la divergencia de Kullback-Liebler sobre la densidad espectral}}\break	\vspace{2cm} 		{\Large Facultad de Ciencias Actuariales}\\		{\Large Alumno: Campos Mart\'inez Joaqu\'in Nicol\'as}\\		{\Large Asesor: Dr. Cuevas Covarrubias Carlos}		\end{center}		\end{titlepage}%=====================================================================================================================% Indice%=====================================================================================================================\tableofcontents %Genera el indice\setcounter{page}{0}%=====================================================================================================================%Resumen ejecutivo%=====================================================================================================================\chapter*{Res\'umen Ejecutivo} \setcounter{page}{0}\chapter{Marco Te\'orico} % (fold)\label{cha:marco_teorico}El t\'itulo del trabajo puede sonar confuso, y rebuscado, ciertamente lo es. Para entender la propuesta que deseamos exponer debemos entender los siguientes conceptos\section{Series de tiempo} % (fold)\label{sec:series_de_tiempo}Una simplificaci\'on de la definici\'on de una serie de tiempo es: un proceso que var\'ia en el tiempo, observaciones tomadas secuencialmente en el tiempo. En la vida real podemos observar varios fen\'omenos de este tipo, en los cambios de temperatura o h\'umedad durante el d\'ia, la ilumnicaci\'on, ejemplos de mayor inter\'es son datos de ventas, precios de acciones, tipos de cambio, clima, medidas biom\'edicas, datos biom\'etricos, rastreo de part\'iculas, etc. \cite{TIME_SERIES_CLUSTERING}\begin{defn}\normalfont  Una serie de tiempo es una coleci\'on de de observaciones hechas en una secuencia de tiempo. Se dice que una serie de tiempo es continua si las observaciones son en tiempos continuos. Se dice discret si cuando las observaciones se toman en tiempos espec\'ificos, usualmente separados a intervalos iguales\end{defn}% section series_de_tiempo (end)\section{An\'alisis de conglomerados} % (fold)\label{sec:analisis_de_conglomerados}<<Clustering is a division of data into groups of similar objects. Each group, called cluster, consists of objects that are similar between themselves and dissimilar to objects of other groups.>>~\cite{CLUSTER_DEFINITION}\\An\'alisis de conglomerados no es una t\'ecnica asociada a un \'unico tipo de problemas. Pero es asociado naturalmente a las ideas de grupos homog\'eneos, clases de equivalencia, datos multimodales.\\Punj \& Stewart identifican algunas de las principales aplicaciones en el campo de investigaci\'on de mercado como segmentaci\'on de mercado, identificaci\'on de grupos homog\'eneos de consumidores, desarrollo de nuevos productos potenciales, selecci\'on de un mercado de prueba, como t\'ecnica general de reducci\'on.\\Las t\'ecnicas aplicadas son varias, y algunas de las m\'as tratadas en la literatura son conglomerados jer\'arquicos, y algunos algoritmos de k-centroides.Tenemos que reconocer que nuestro objeto de estudio son observaciones multivariadas, medidas en $p$ variables, que pueden ser categ\'oricas (respuestas a una encuesta, s\'i, no) o num\'ericas (valores continuos, como estatura y peso)\subsection{Conglomerados Jer\'arquicos} % (fold)\label{sub:conglomerados_jerarquicos}Se separa en 2 grandes categor\'ias, aglomerativos y divisivos. El primer tipo considera cada observaci\'on, elemento de la muestra, como un conglomerado y comienza a agruparlos basado en alguna regla. El segundo comienza con toda la muestra como un gran conglomerado y comienza a separarlo en distintos conglomerados, en su mayor\'ia conforme a una funci\'on objetivo. Un an\'alisis de conglomerados jer\'arquico es sensible, o depende principalmente de 2 factores.\begin{itemize}	\item La medida de \textbf{similitud o distancia} elegida	\item El algoritmo para agrupar elementos, la medida de \textbf{distancia o similitud} entre conglomerados\end{itemize}Conglomerados jer\'arquicos aglomerativos suelen ser los t\'ipicos ejemplos de libro de texto por su facilidad para explicarse, sin embargo esto no significa que su utilidad sea menor.Primero debemos abordar el tema de distancia o simulitud. \begin{defn}\normalfont	Dado un conjunto $X$ una medida de distancia es una funci\'on $d:X\times X\rightarrow\Reales$ tal que cumple las siguientes propiedades	\begin{itemize}		\item $d(x, y) = 0$ si y s\'olo si $x = y$		\item $d(x, y) = d(y,x)$ para todas $x, y \in X$		\item $d(x, y) + d(y, z) \geq d(x, z)$ para todas $x, y, z \in X$	\end{itemize}	La propiedad de no negatividad $d(x, y) \geq 0$ para todas $x, y \in X$ queda definida por las propiedades anteriores.\end{defn}La idea de distancia suele ser intuitiva, y se asocia com\'unmente con la distancia euclidiana, sin emabargo existen muchas otras formas de distancia, como la suma de valores absoulutos de las componenetes. En cambio, la simulitud, aunque la idea es intuitiva, no suele ser tan clara cuando uno maneja valores num\'ericos o cateog\'oricos.Exiten algunos intentos sobre la definici\'on de una medida de similitud, entre ellos mencionaremos como ejemplo la definic\'on presentada por Chen, Ma y Zhang ~\cite{CLUSTER_ANALYSIS_2}\begin{defn}\normalfont	Dado un conjunto $X$ una medida de similitud es una funci\'on $s:X\times X\rightarrow\Reales$ que cumple las siguientes propiedades	\begin{itemize}		\item $s(x, y) = s(y, x)$ para todo $x, y\in X$		\item $s(x, x) \geq 0$ para todo $x\in X$		\item $s(x, x) \geq s(x, y)$ para todo $x, y\in X$		\item $s(x, y) + s(y, z) \leq s(x, z) + s(y, y)$ para todo $x, y, z \in X$		\item $s(x, x) = s(y,y) = s(x, y)$ si y s\'olo si $x=y$	\end{itemize}\end{defn}La propiedad de simetr\'ia es intuitiva, se espera que un objeto sea tan similar a otro, como el otro al primero. La segunda propiedad no es necesaria, pero su importancia radica en el concepto que tenemos de similitud, un objeto no puede tener similitud negativa con respecto de si mismo. La tercer propiedad parte de la idea de que un ning\'un obteto es tan similar a otro como a s\'i mismo. La cuarta propiedad ser\'ia el equivalente a la desigualdad del tri\'angulo, y aunque a primera vista parece poco comprensible, es m\'as f\'acil analogarla con la idea de intersecci\'on de conjuntos, podemos decir que $s$ es una medida de todo lo que tienen en com\'un 2 objetos $x, y$. La \'ultima propiedad nos dice que s\'olo cuando 2 objetos son iguales se alcanzan las cuotas superiores dela similitud.No es importante recordar esta definici\'on de similitud, pues nosotros trabajaremos algo m\'as parecido a una distancia que a una similitud.Lo siguiente es construir una matriz de distancias o similitudes entre los objetos. Si denotamos $s_{ij} = s(x_i, x_j)$ y $d_{ij} = d(x_i, x_j)$, y tenemos un total de $n$ observaciones, nuestra matriz de similitud es $$	\left[\begin{array}{cccc}		s_{11} &  s_{12} & \cdots & s_{1n}\\		s_{21} &  s_{22} & \cdots & s_{2n}\\		\vdots & \vdots & \ddots & \vdots\\		s_{n1} &  s_{n2} & \cdots & s_{nn}\\	\end{array}\right]$$y nuesta matriz de distancias es $$	\left[\begin{array}{cccc}		d_{11} &  d_{12} & \cdots & d_{1n}\\		d_{21} &  d_{22} & \cdots & d_{2n}\\		\vdots & \vdots & \ddots & \vdots\\		d_{n1} &  d_{n2} & \cdots & d_{nn}\\	\end{array}\right]$$En un algoritmo aglomerativo el siguiente paso es juntar las 2 observaciones, distintas, m\'as similares (menos distantes) para formar un conglomerado, luego se recalcula la distancia del resto de los conglomerados (observaciones) al conglomerado reci\'en formado. \textbf{¿C\'omo se calcula la distancia entre 2 conglomerados?}. Existen distintas reglas para calcular la distancia, algunas de las m\'as conocidas son\begin{itemize}	\item Vecino m\'as cercano (single linkage), es decir, la distancia entre 2 conglomerados es la distancia m\'as corta de sus elementos	\item Vecino m\'as lejano (complete linkage), es decir, la distancia entre 2 conglomerados es la distancia m\'as grande entre sus elementos	\item Promedio (average linkage), media de las distancias entr los elementos de ls conglomerados.	\item Centroide, se obtiene el centro de masa de los conglomerados y se calcula la distancia entre los mismo\end{itemize}Este proceso se repite hasta tener toda la muestra en un conglomerado, y cuando uno decida hacer el corte, cuando hay $r$ conglomerados, donde el usuario escoge $r$.% subsection conglomerados_jerarquicos (end)\subsection{K-centroides} % (fold)\label{sub:k_centroides}Los algoritomos de k-centroides est\'an basados en la minimizaci\'on de una funci\'on de suma de cuadrados, es decir la suma de cuadrados de la distancia de los elementos de un conglomerado a su centroide.En comparaci\'on con un an\'alsis jer\'arquico, k-centroides depende de 2 configuraciones\begin{itemize}	\item La primera es el n\'umero de conglomerados, $k$	\item El tipo de centroide, centro de gravedad, medoide.\end{itemize}  En el caso de \textbf{k-means}, el centroide es el centro de gravedad y   $$\begin{array}{ccc}E &=& \sum_{h=1}^k \sum_{i=1}^n u_{i,h} d(x_i, \mu_h)^2\end{array}$$Es la funci\'on que deseamos optimizar~\cite{CLUSTERING_1}. Donde los $u_{ih}$ clumplen que $\sum_{h=1}^k u_{ih}=1$ para $i = 1, \dots, n$ y $\sum_{i=0}^n u_{ih}=1$ para $h = 1, \dots, k$, y $\mu_h$ es la la media del $h$-\'esimo conglomerado. En el caso de medoides sustituimos $\mu_h$ por $m_h$, donde $m_h$ es el medoide del $h$-\'esimo.Hasta ahora s\'olo hemos presentado metodolog\'ias en las que una observaci\'on no puede ser parte de m\'as de un conglomerado sin embargo existen otros algoritmos que asignan «grados de pertenencia» en lugar de un conglomerado fijo.% subsection k_centroides (end)% section analisis_de_conglomerados (end)\section{An\'alisis espectral} % (fold)\label{sec:analisis_espectral}Inferencias basadas en la funci\'on de densidad espectral se dice que es un an\'alisis in el Dominio de las frecuencias.\begin{teorema}[Wiener-Khintchine]\normalfontPara cualquier proceso estoc\'astico estacionario con funci\'on de autocovarianza $\gamma(k)$ existe una funci\'on monotamente creciente $F(W)$ tal que $$\gamma(k) = \int^\pi_0 \cos(wk)dF(w)$$Llamada la representaci\'on espectral de la funci\'on de autocovarianza, que involucra un tipo de integral llamada de Stieltjes\end{teorema}$F(w)$ tiene una interpretaci\'on f\'isica directa, es la contribuci\'on a la varianza que se puede atribuir a las frecuencias en el rango $(0,w)$$F(w)$ es mon\'otona en el intervalo $(o,\pi)$, as\'i se puede descomponer en 2 funciones $F_1(w)$ y $F_2(w)$ talque $$F(w) = F_1(w) + F_2(w) $$ Donde $F_1(w)$ es una funci\'on continua no decreciente y $F_2(w)$ es una funci\'on escalonada no decreciente(WOLD descomposition). Adem\'as $F_1(w)$ se relaciona con la componente puramente no determinista del proceso y $F_2(w)$ se relaciona con la componente determinista. Nuestro estudio es sobre los procesos puramente indeterminados, donde $F_2(w) = 0$, de tal forma que $F(w)$ es continua en $(0, \pi)$.La potencia est\'a directamente relacionado al cuadrado de la amplitud de oscilaci\'on. y la varianza es la potencia total\begin{defn}\normalfontLa forma normalizada de $F(w)$ est\'a dada por$$F^* = \frac{F(w)}{\sigma_x^2}$$De modo que $F^*(w)$ es la proporci\'on de varianza atribuida por las frecuencias en el rango $(0,w)$. Como $F^*(\pi) = 1$ y es adem\'as mon\'ona creciente tenemos que $F^*(w)$ tiene propiedades similares a una funci\'on de distribuci\'on acumulada.\end{defn}% section analisis_espectral (end)\section{Kullback-Liebler} % (fold)\label{sec:kullback_liebler}\begin{defn}\normalfont	Dadas dos funciones de densidad de probabilidad $f, g$ definidas sobre un mismo espacio medible, y que son absolutamente continuas entre s\'i, la divergencia de Kullback-Liebler se define como	$$		d_{KL}(f, g) = \int f(x) \ln{\left(\frac{f(x)}{g(x)}\right)}dx	$$\end{defn}La divergencia de Kullback-Liebler como una medida de divergencia dirigida. Una manera de entender lo anterio es primero considerar un modelo verdadero $f$ y un modelo de aproximaci\'on $g$, entonces la divergencia de Kullback-Liebler nos habla de cuanta informaci\'on se pierde utilizando $g$ como aproximaci\'on ~\cite{KULLBACK}. La divergencia de Kullback-Liebler no cumple con las propiedades de distancia mencionadas anteriormente.La propiedad m\'as cercana a una distancia, es la positividad y la nulidad cuando $f$ y $g$ son iguales exceptuando en puntos de probabilidad $0$.Ahora podemos definir la divergencia de Jeffreys\begin{defn}\normalfont	Dadas dos funciones de densidad de probabilidad $f, g$ definidas sobre un mismo espacio medible, y que son absolutamente continuas entre s\'i, la divergencia de Jeffreys se define como	$$		d_{J}(f, g) = d_{KL}(f, g) +d_{KL}(g, f)	$$\end{defn}En este caso, tenemos dos modelos, y la divergencia de Jeffreys nos indica que tan la dificultad para distinguir entre ambos~\cite{KULLBACK_2}'% section kullback_liebler (end)% chapter marco_teorico (end)%=====================================================================================================================%Chapter 1 {An\'alisis de conglomerados}%=====================================================================================================================\chapter{An\'alisis de conglomerados} % Representing data by fewer clusters necessarily loses certain fine details (akin to lossy data compression), but achieves simplification. It represents many data objects by few clusters, and hence, it models data by its clusters.Data modeling puts clustering in a historical perspective rooted in mathematics, statistics,and numerical analysis. From a machine learning perspective clusters correspond to hidden patterns, the search for clusters is unsupervised learning, and the resulting system represents a data concept. Therefore, clustering is unsupervised learning of a hidden data concept. Data mining deals with large databases that impose on clustering analysis additional severe computational requirements. These challenges led to the emergence of powerful broadly applicable data mining clustering methods surveyed below.%=====================================================================================================================%Section 1.1 {Tema}%=====================================================================================================================\section{Tema}El tema general es \textit{an\'alisis de conglomerados}, con un enfoque a \textbf{series de tiempo}. Es decir la agrupaci\'on de series de tiempo en diferentes grupos seg\'un su <<similitud>>. \\\\Una de las principales motivaciones fue el plantemiento de un problema de unsupervised pattern recognition, el cual consiste en lo siguiente. Dado un grupo de canciones, deseamos crear listas de reproducci\'on que suenen bien. Por sonar bien, queremos decir que las listas de reproducci\'on presenten <<sinergia>>.\\\\% Nuestro acercamiento consiste en tomar las funci\'on de \textbf{densidad espectral} varias series de tiempo, o una fracci\'on de las mismas; y luego aplicar la divergencia de \textbf{Kullback-Liebler} como m\'edida de distancia entre las funciones de densidad. Y luego aplicar alg\'un algoritmo jerarquico de aglomeraci\'on.% Podemos interpretar a la m\'usica como una serie de tiempo, y podmeos realizar un acercamiento por metodolog\'ias aplicables a series de tiempo. Cabe mencionar esta no es la \'unica aplicaci\'on. %=====================================================================================================================%Section 1.2 {Objetivo}%=====================================================================================================================\section{Objetivo}Deseamos probar la calidad de una metodolog\'ia para conlglomerar series de tiempo basada en \textbf{an\'alisis espectral} y la \textbf{divergencia de Kullback-Liebler} como medida de distancia entre series de tiempo. En nuestro caso se utilizar\'a un m\'etodo de conglomeraci\'on jer\'arquico. Y deseamos conocer si este enfoque posee ventajas intr\'insecas sobre otras metodolog\'ias de conlgomeraci\'on de series de tiempo, o si revela informaci\'on relevante sobre las series de tiempo.%Mejora Buscar sobre pruebas de calidad sobre los clusters%=====================================================================================================================%Section 1.3 {Relevancia}%=====================================================================================================================\section{Relevancia}El an\'alisis de conglomerados suele ser uno de los primero acercamientos en el an\'alisis exploratorio. Por otro lado, las series de tiempo pueden ser observadas en diferentes \'ambitos informaci\'on de ventas, precios de acciones, tasas cambiarias, informaci\'on sobre el clima, datos biom\'edicos, etc. ~\cite{TIME_SERIES_CLUSTERING}\\\\Las aplicaciones de an\'alisis de conglomerados en estos \'ambitos son variadas, por ejemplo, en medicina es importante reconocer la diferencia entre se\~nales normales en un ECG, EEG, EMG de aquellas producidas por enfermedades. En sismolog\'ia, para discriminar las ondas s\'ismicas, de los moviemientos normales de la tierra~\cite{TIME_SERIES_CLUSTERING_2}. Otras aplicaciones son, en astronom\'ia las curvas de luz muestran el brillo de una estrella en un periodo de tiempo, en medicina la actividad cerebral, en el medio ambiente y urbanizaci\'on los niveles de la marea, niveles de contaminantes en el aire ($PM_{2.5}, PM_{10}$)~\cite{TIME_SERIES_CLUSTERING}.%=====================================================================================================================%Chapter 2 {Antecedentes}%=====================================================================================================================\chapter{Antecedentes}Hay m\'ultiples art\'iculos sobre an\'alisis de conglomerados, y algunos de ellos se enfocan en series de tiempo. \textit{Time-series clustering - A decade review}~\cite{TIME_SERIES_CLUSTERING} tiene como objetivo comparar los enfoques m\'as populares.\\<<This review will expose four main components of time-series clustering and is aimed to represent an updated investigation on the trend of improvements in efficiency, quality and complexity of clustering time-series approaches during the last decade and enlighten new paths for future works>>~\cite{TIME_SERIES_CLUSTERING}\\\\\textit{Clustering of time series data - a survey} busca resumir investigaci\'on realizada en el tema y sus aplicaciones en varios campos. Incluye aspectos basicos de los algoritmos m\'as generales que se utilizan en estudios de conglomerados, medidas de similitud y disimilitud, evaluaci\'on de conglomerados.~\cite{TIME_SERIES_CLUSTERING_3}\\\\Ambos art\'iculos distinguen 3 principales categorizaciones de los acercamientos a conglomeraci\'on de series de tiempo, trabajar con los datos brutos, extraer <<caracter\'isticas>> de los datos, basarse en un modelo al que se ajusten los datos. \\\\<<Computational Models of Music Similarity and their Application in Music Information Retrieval>> es una tesis que busca mostrar el desarrollo de las t\'ecnicas de descubrimiento de m\'usica basado en medidas de similitud y disimilitud. Uno de los enfoques consiste en utilizar de funci\'on de densidad espectral. Sin embargo tambi\'en utiliza t\'ecnicas aplicables a series de tiempo.~\cite{MUSIC}%=====================================================================================================================%Chapter 3 {Metodolog\'ia}%=====================================================================================================================\chapter{Metodolog\'ia}Primero introduciremos el concepto de an\'alisis de conglomerados, motivaciones para su estudio, los algoritmos m\'as populares, y su uso en nuestra \'area de inter\'es.\\\\Luego hablaremos sobre los conceptos de an\'alisis espectral, funci\'on de densidad espectral, como aproximarla apartir de datos, interpretaci\'on y uso. Luego estudiaremos la divergencia de Kullback-Liebler, indicaremos sus caracter\'isitcas y sus fallos como medida de distancia, introduciremos una forma de simetrizarla, y buscaremos probar que cumpla la desigualdad del tri\'angulo.\\\\Luego procederemos a probar este enfoque, mediante la selecci\'on de m\'utiples algoritmos jer\'arquicos de conglomeraci\'on. Luego escogeremos un tipo de dato de serie de tiempo para aplicar el an\'alisis, despu\'es utilizaremos pruebas sobre los conglomerados obtenidos para verificar su calidad. Despu\'es compararemos con otras metodolog\'ias disponibles en paquetes o repositorios.%=====================================================================================================================%Chapter 4 {Fuentes de datos}%=====================================================================================================================\chapter{Fuentes de datos}La cantidad de informaci\'on disponible actualmente es impresionante, y se puede obtener de distintas formas, mediante bases de datos online, minando informaci\'on, o simplemente checando el registro de nuestras acciones. \\\\Para efectos de este estudio, los datos se obtendr\'an de distintas fuentes, principalmente de archivos de audio, mp3, representando m\'usica obtenidos mediante compra en distintos servicios iTunes Store\textcopyright, Bandcamp\textcopyright, o adquiridos gratuitamente. Los datos se extraer\'an mediante con la ayuda de 2 librer\'ias del R <<tuneR>> y <<seewave>>. Las cu\'ales contienen funcionalidad para trabajar con ondas, archivos de m\'usica, etc. Otra fuente es los precios de acciones en adquiridos mediante Yahoo Finance \textcopyright\\\\%=====================================================================================================================%Chapter 5 {An\'alisis y desarrollo}%=====================================================================================================================\chapter{An\'alisis y desarrollo}\section{Introducci\'on a an\'alisis de conglomerados}Como hab\'iamos mencionado an\'alisis de conglomerados es una t\'ecnica cuyo objetivo es agrupar en m\'ultiples clases seg\'un sus caracter\'isticas.\\\\Existen distintas metodolog\'ias. Las m\'as simples y conocida son las jer\'arquicas. Que se subdividen en 2 grupos divisivas y aglomerativas.% ReferenceLos algoritmos divisivos, comienzan con todos los objetos en un mismo conglomerado, y comienzan a separarlo utilizando un criterio de optimizaci\'on, y continua hasta separar todos los objetos. Los algoritmos aglomerativos, trabajan en sentido contrario, toman cada objeto como un conglomerado separado, y comienza a agrupar los conglomerados m\'as cercanos basado en una regla de distancia o similitud entre conglomerados.\\\\%=====================================================================================================================%Chapter 6 {Expectativas Practicum II}%=====================================================================================================================\chapter{Expectativas Practicum II}%=====================================================================================================================%Chapter 6 {Plan de trabajo}%=====================================================================================================================\chapter{Plan de trabajo}\section{Horario}\begin{tabular}{ c | c }\textbf{D\'ias} & \textbf{Horario} \\Mi\'ercoles & 10:00 - 13:00  \\Jueves & 16:00 - 19:00  \\Viernes & 12:00 - 13:00  \\\end{tabular}\section{Avances}\begin{tabular}{|p{2cm}|p{4cm}|p{12cm}|}\textbf{Estatus} & \textbf{D\'a} & \textbf{Avance} \\ No terminado & 18, Septiembre, 2017 & Resumen y an\'alisis  de la lectura \textit{Time-series clustering – A decade review} \\ \hline\\ No terminado & 20, Septiembre, 2017 & Primer escrito sobre el tema, las herramientas disponibles, procedimientos aplicables y obtenci\'on de informaci\'on\\ \hline\\ No iniciado & 25, Septiembre, 2017 & Resumen y an\'alisis  de la lectura \textit{PATTERN CLUSTERING BY MULTIVARIATE MIXTURE ANALYSIS}\\ \hline\\ No iniciado & 27, Septiembre, 2017 & Presentaci\'on con primeros avance y pruebas\\ \hline\\ No iniciado & 14, Septiembre, 2017 & Resumen y an\'alisis  de la lectura \textit{Overlapping Clustering: A New Method for Product Positioning}\\ \hline\\ No iniciado & 02, Obtubre, 2017 & Breve Resumen y an\'alisis  del libro \textit{Data Clustering Theory, Algorithms, and Applications}\\ \hline\\ No iniciado & 09, Octubre, 2017 & Resumen y an\'alisis  de la lectura \textit{Clustering of time series data — a survey}\\ \hline\\ No iniciado & 16, Obtubre, 2017 & Resumen y an\'alisis  de la lectura \textit{Latent class models for clustering: A comparison with K-means}\\ \hline\\ No iniciado & 18, Octubre, 2017 & Segundo excrito sobre el comportamiento de los datos, y los resultados de utilizar los m\'etodos cl\'asicos, presentaci\'on de resultados, ventajas y desventajas\\ \hline\\ Pr\'oximamente & Pr\'oximamente\\\end{tabular}%=====================================================================================================================%Chapter  {Conclusiones}%=====================================================================================================================\chapter*{Conclusiones}%=====================================================================================================================%Chapter  {Agregar}%=====================================================================================================================\chapter*{Agregar}Las metodolog\'ias de k-centroides parten de optimizar, minimizar, al una funci\'on objetivo. El primer paso es definir el n\'umero de conglomerados, a diferencia de los algoritmos jer\'arquicos, k-centroides requiere este valor para comenzar, despu\'es necesita una configuraci\'on inicial, ya sea escoger los k-centroides o definir conglomerados aleatorios y obtener sus centroides. \section{HCM, Hard-C-Means} En este caso los centroides son las medias aritm\'eticas de los valores de los elementos en el conglomerado. Digamos que tenemos $n$ objetos, y escogemos m  podemos definir que $\mu_h$ es la media de los valores en el \textit{h-\'esismo} conglomerado, y $d(x, y)$ una funci\'on de distancia adecuada para los objetos que estamos estudiando. As\'i$$\begin{array}{ccc}E &=& \sum_{h=1}^m \sum_{i=1}^n u_{i,h} d(x_i, \mu_h)^2\end{array}$$Es la funci\'on que deseamos optimizar~\cite{CLUSTERING_1}. Donde los $U_{ih}$ clumplen que $\sum_{h=0}^m u_{ih}=1$ para $i = 1, \dots, n$ y $\sum_{i=0}^n u_{ih}=1$ para $h = 1, \dots, m$.\\\\En este caso $u_{ih}$ s\'olo puede estar $\{0, 1\}$\subsection{FCM, Fuzzy k-Means} % (fold)\label{sub:fcm_fuzzy_k_means}En HCM podemos notar que se forma una partici\'on de la muestra, y un objeto no puede pertenecer a m\'as de un conglomerado. Por otro lado podemos cosiderar que los elementos de la muestra pueden ser parte de m\'as d un conglomerado, pero puede tener <<grados de pertenencia>>~\cite{CLUSTERING_1}.\\\\En este caso la funcion a minimizar es $$\begin{array}{ccc}E &=& \sum_{h=1}^m \sum_{i=1}^n u_{i,h}^\alpha d(x_i, \mu_h)^2\end{array}$$Donde $\alpha\in\Reales^+$ es llamado el <<\textit{fuzzifier}>>~\cite{CLUSTERING_1}. Las condiciones que los $u_{ih}$ deben cumplir son las mismas, pero ahora $u_{ih}$ est\'an en el intervalo $[0,1]$Existen otras metodolog\'ias, pero de momento adentremonos en otro tema.% subsection fcm_fuzzy_k_means (end)\chapter{M\'usica} % (fold)\label{cha:musica}Primero debemos  obtener nuestro material para trabajar, en nuestro caso nuestro principal objeto es la m\'usic a y su representaci\'on como serie de tiempo. As\'i haremos uso archivos de mp3 comprados, o gratuitos para nuestra materia prima.\\\\Para procesar la informaci\'on utilizamos las librer\'ias \textit{tuneR} y \textit{seewave} de R. De este mode se obtiene una serie de tiempo multivariada de 2 dimensiones, audio estereo, una por canal de saalida, R (Derecha), L(Izquierda). Para simplificar el trabajo reducimos la informaci\'on a un s\'olo canal, audio mono.\\\\Para procesar estos datos y obtener informaci\'on se pueden realizar distintos procesos o acercamientos, los 3 principales son \textit{Similitud en Tiempo}, donde los calculos son en cada paso de las series de tiempo, siendo as\'i computacionalmente costos, <<Insert From Lectures>>. \emph{Similar en forma}, no se utiliza la ubicaci\'on temporal de los objetos, sino su forma. \emph{Similar en Cambio} Cuando las series de tiempo presentan patrones parecidos como ca\'idas empinandas despu\'es de un crecimineto constante.\emph{Insertar gr\'aficas con ejemplos'}\\\\ Pero para nuestro estudio nos enfocaremos en un m\'etodo de tipo, an\'alisis espectral, que podr\'iamos decir es la versi\'on estoc\'astica de an\'alisis de Fourier.\\\\En el an\'alsis de series de tiempo, com\'unmente tenemos un problema de \emph{trade-off} entre la calidad de la informaci\'on procesada y la velocidad del proceso. Raz\'on por la que no parece posible utilizar al 100\% todos los datos de las series de tiempo.\emph{Tarea, investigar sobre las ventajas del An\'alisis espectral y que son Midi notes'}% Para nuestro estudio nos interesa conocer si la divergencia de Lulbac-Lieber o una modificaci\'on de la misma puede ser utilizada como distancia entre densidades espectrales'% chapter musica (end)\chapter{An\'alisis de conglomerados} % (fold)\label{cha:analisis_de_conglomerados}Introducir lo que se puso arriba mejor acomodado% chapter analisis_de_conglomerados (end)\chapter{An\'alisis espectral} % (fold)\label{cha:analisis_espectral}% Investigar sobre este tema y an\'alisis de fourier'% \\\\% <<Wikipediazo>> Nos dice que cualquier se\~nal f\'isica se puede descomponer en un n\'umero de frecuencias discretos, o en un espectro de frecuencias sobre un rango continuo.% \\\\% Caundo la energ\'ia de la se\~nal se concentra alrededor de un intervalo finito de tiempo, si su energ\'ia total es finita entonces podemos calcular la densidad espectral.% \\\\% La suma o integraci\'on  de los componenetes espectrales El espectro de un proceso f\'isico $x(t)$ suele contener informaci\'on esencial sobre la naturaleza de X, (pitch and timbre of musica instruments)An\'alisis espectral es b\'asicamente una modificaci\'on del an\'alisis de Fourier para hacerlo aplicable a funciones estoc\'asticas del tiempoDrichlet conditions'\\\\Involucra la tranformada de Fourier, y generalizaci\'on basada en an\'alisis de Fourier primero distinguimos algunas caracter\'isiticas esenciales de una serie de tiempo (Chatfield)'% chapter analisis_espectral (end)\chapter{Series de Tiempo} % (fold)\label{cha:series_de_tiempo}\begin{defn}  Una serie de tiempo es una coleci\'on de de observaciones hechas en una secuencia de tiempo. Se dice que una serie de tiempo es continua si las observaciones son en tiempos continuos. Se dice discret si cuando las observaciones se toman en tiempos espec\'ificos, usualmente separados a intervalos iguales\end{defn}De los 4 objetivos usuales del an\'alisis de series de tiempo el que nos interesa m\'as es descripci\'on (y explicaci\'on)\\\\<<Anyone who tries to analyse a time series without plotting it first, is asking for trouble>>'<<Autocovariance (or autocorrelation) function is the natural tool for considering the evolution of a process through time. As spectral density function is the natural tool for considering the frequencia properties of a time series>>\\\\Inferencias basadas en la funci\'on de densidad espectral se dice que es un an\'alisis in el Dominio de las frecuencias.\begin{teorema}[Wiener-Khintchine]Para cualquier proceso estoc\'astico estacionario con funci\'on de autocovarianza $\gamma(k)$ existe una funci\'on monotamente creciente $F(W)$ tal que $$\gamma(k) = \int^\pi_0 \cos(wk)dF(w)$$Llamada la representaci\'on espectral de la funci\'on de autocovarianza, que involucra un tipo de integral llamada de Stieltjes\end{teorema}$F(w$ tiene una interpretaci\'on f\'sica directa, es la contribuci\'on a la varianza que se puede atribuir a las frecuencias en el rango $(0,w)$$F(w)$ es mon\'otona en el intervalo $(o,\pi)$, as\'i se puede descomponer en 2 funciones $F_1(w)$ y $F_2(w)$ talque $$F(w) = F_1(w) + F_2(w) $$ Donde $F_1(w)$ es una funci\'on continua no decreciente y $F_2(w)$ es una funci\'on paso(?) no decreciente(WOLD descomposition). Adem\'as $F_1(w)$ se relaciona con la componente puramente no determinista del proceso y $F_2(w)$ se relaciona con la componente determinista. Nuestro estudio es sobre los procesos puramente indeterminados, donde $F_2(w) = 0$, de tal forma que $F(w)$ es continua en $(0, \pi)$EL poder est\'a directamente relacionado al cuadrado de la amplitud de oscilaci\'on. y la varianza es el poder total\begin{defn}La forma normalizada de $F(w)$ est\'a dada por$$F^* = \frac{F(w)}{\sigma_x^2}$$De modo que $F^*(w)$ es la proporci\'on de varianza atribuidad por las frecuencias en el rango $(0,w)$. Como $F^*(\pi) = 1$ y es adem\'as mon\'ona creciente tenemos que $F^*(w)$ tiene propiedades similares a una funci\'on de distribuci\'on acumulada\end{defn}Si el tama\~no de nuestra poblaci\'on es $n<20$, nuestro estad\'istico de prueba es$$T = B $$Si el tama\~no de nuestra poblaci\'on es $n\geq0$, nuestro estad\'istico de prueba es$$T=\frac{(B-C)^2}{B+C}$$La regi\'on de rechazo es $$\begin{array}{ccc}[0,bin_{\frac{\alpha}{2}}] \cup [bin_{1-\frac{\alpha}{2}},b+c]\end{array}$$$$(z_{1-\alpha},\infty)$$$$p-val = 2\mathbb{P}(T\leq t)$$$$p-val = 2\mathbb{P}(T\geq t)$$$$p-val = \mathbb{P}(T\geq t)$$Una modificaci\'on usual, es cuando las frecuecias esperadas son peque\~nas, de modo que la aproximaci\'on es pobre, de modo que se aplica la modificaci\'on de Yates$$T=\frac{(|B-C|-1)^2}{B+C}$$$$ \approx\chi^2_{(1)}$$% chapter series_de_tiempo (end)  %=====================================================================================================================%Bibliograf\'ia%=====================================================================================================================\bibliography{Tar}{}\bibliographystyle{apacite}% \begin{thebibliography}{9}% \bibitem{CAST}%   Saeed Aghabozorgi, Ali Seyed Shirkhorshidi, Teh Ying Wah,%   2014,%   \textit{Time-series clustering – A decade review},%   Obtenido de \url{http://www.sciencedirect.com/science/article/pii/S0306437915000733?via%3Dihub}% \bibitem{CAD}%   Pradeep Rai, Shubha Singh,%   2010,%   \textit{A Survey of Clustering Techniques},%   Obtenido de \url{http://www.ijcaonline.org/volume7/number12/pxc3871808.pdf}% \end{thebibliography}\end{document}